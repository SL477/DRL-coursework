{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TawK-YKMM4IU"
   },
   "source": [
    "# Soft-Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1650810858064,
     "user": {
      "displayName": "Thomas Fishwick",
      "userId": "17375840183006086804"
     },
     "user_tz": -60
    },
    "id": "eecURk7tM4IY"
   },
   "outputs": [],
   "source": [
    "# From Computer vision lab 6, keeps reloading the code for the objects so that you can work on them without needing to reload the kernal\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "SAVE_PATH=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16169,
     "status": "ok",
     "timestamp": 1650810876999,
     "user": {
      "displayName": "Thomas Fishwick",
      "userId": "17375840183006086804"
     },
     "user_tz": -60
    },
    "id": "a3Lx4MyuVb-j",
    "outputId": "6d97d5b1-46e2-42cc-9acd-1d2a33ec3e57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# for colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "path = '/content/drive/My Drive/Colab Notebooks/DRL/DRL Coursework/'\n",
    "\n",
    "import sys\n",
    "sys.path.append(path)\n",
    "\n",
    "SAVE_PATH=path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 9082,
     "status": "ok",
     "timestamp": 1650810888374,
     "user": {
      "displayName": "Thomas Fishwick",
      "userId": "17375840183006086804"
     },
     "user_tz": -60
    },
    "id": "7IPqj2CMM4IZ"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from helpers.get_available_actions import get_available_actions, adv_action_from_index\n",
    "import numpy as np\n",
    "from helpers.advanced_map import AdvancedMap\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "import pickle\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# determine if we are using cpu or gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 287,
     "status": "ok",
     "timestamp": 1650810891875,
     "user": {
      "displayName": "Thomas Fishwick",
      "userId": "17375840183006086804"
     },
     "user_tz": -60
    },
    "id": "AG1Or5WSM4Ib"
   },
   "outputs": [],
   "source": [
    "# tuple from page 3, part 3.1 of https://arxiv.org/pdf/1801.01290.pdf\n",
    "# state space\n",
    "# action space\n",
    "# state_transition_probability is initially unknown, s*s*a\n",
    "# reward amount\n",
    "MDP_tuple = namedtuple('MDP_tuple', ['state', 'action', 'next_state', 'reward', 'done'])\n",
    "# from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 258,
     "status": "ok",
     "timestamp": 1650810895230,
     "user": {
      "displayName": "Thomas Fishwick",
      "userId": "17375840183006086804"
     },
     "user_tz": -60
    },
    "id": "Lss9uqw2M4Ic"
   },
   "outputs": [],
   "source": [
    "# Build the Deep Q Network\n",
    "class DQN(torch.nn.Module):\n",
    "    \"\"\"Deep Q Network, sizes based upon the grid search in RLLib\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Create an instance of the network\"\"\"\n",
    "        # Call the base objects init method\n",
    "        super().__init__()\n",
    "\n",
    "        # our advanced network will be sending in a 53 column list of numbers\n",
    "        self.dense1 = torch.nn.Linear(53, 256)\n",
    "        # normalise the data in the network\n",
    "        self.norm1 = torch.nn.BatchNorm1d(256)\n",
    "\n",
    "        self.dense2 = torch.nn.Linear(256, 256)\n",
    "        self.norm2 = torch.nn.BatchNorm1d(256)\n",
    "\n",
    "        self.dense3 = torch.nn.Linear(256, 256)\n",
    "        self.norm3 = torch.nn.BatchNorm1d(256)\n",
    "\n",
    "        # 5 potential actions\n",
    "        self.dense4 = torch.nn.Linear(256, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass the data through the network\"\"\"\n",
    "        # convert the input data to a float, run it through the first dense layer, then through the first batch norm layer and then apply the relu function\n",
    "        output = torch.functional.F.relu(self.norm1(self.dense1(x.float())))\n",
    "        # repeat for \n",
    "        output = torch.functional.F.relu(self.norm2(self.dense2(output)))\n",
    "\n",
    "        output = torch.functional.F.relu(self.norm3(self.dense3(output)))\n",
    "        # reshape the tensor and pass it through the last stage\n",
    "        return self.dense4(output.view(output.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 265,
     "status": "ok",
     "timestamp": 1650810939379,
     "user": {
      "displayName": "Thomas Fishwick",
      "userId": "17375840183006086804"
     },
     "user_tz": -60
    },
    "id": "Yg6YsEJXT_-S"
   },
   "outputs": [],
   "source": [
    "class Pi(torch.nn.Module):\n",
    "    \"\"\"Need to work out whether to explore or not\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Create an instance of the network\"\"\"\n",
    "        # Call the base objects init method\n",
    "        super().__init__()\n",
    "        # our advanced network will be sending in a 53 column list of numbers\n",
    "        self.dense1 = torch.nn.Linear(53, 256)\n",
    "        # normalise the data in the network\n",
    "        self.norm1 = torch.nn.BatchNorm1d(256)\n",
    "\n",
    "        self.dense2 = torch.nn.Linear(256, 256)\n",
    "        self.norm2 = torch.nn.BatchNorm1d(256)\n",
    "\n",
    "        self.dense3 = torch.nn.Linear(256, 256)\n",
    "        self.norm3 = torch.nn.BatchNorm1d(256)\n",
    "\n",
    "        # 2 potential policies, exploit or explore\n",
    "        self.dense4 = torch.nn.Linear(256, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass the data through the network\"\"\"\n",
    "        # convert the input data to a float, run it through the first dense layer, then through the first batch norm layer and then apply the relu function\n",
    "        output = torch.functional.F.relu(self.norm1(self.dense1(x.float())))\n",
    "        # repeat for \n",
    "        output = torch.functional.F.relu(self.norm2(self.dense2(output)))\n",
    "\n",
    "        output = torch.functional.F.relu(self.norm3(self.dense3(output)))\n",
    "        # reshape the tensor and pass it through the last stage\n",
    "        return torch.sigmoid(self.dense4(output.view(output.size(0), -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1650810899609,
     "user": {
      "displayName": "Thomas Fishwick",
      "userId": "17375840183006086804"
     },
     "user_tz": -60
    },
    "id": "dUM2IOPQM4Id"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \"\"\"from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        # uses a deque structure so that as new items are added to the end of the 'list' items at the front are pushed off of it\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(MDP_tuple(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Return a random sample from the memory\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Clear the memories\"\"\"\n",
    "        # reset the memory\n",
    "        self.memory = deque([], maxlen=self.memory.maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1650810901876,
     "user": {
      "displayName": "Thomas Fishwick",
      "userId": "17375840183006086804"
     },
     "user_tz": -60
    },
    "id": "PJOxw1xdM4Id"
   },
   "outputs": [],
   "source": [
    "def select_action(state, steps_done, policy) -> torch.tensor:\n",
    "    \"\"\"Inspired by https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html.\n",
    "    Takes in the current state and the number of steps and returns an action either from the network or a random action\"\"\"\n",
    "\n",
    "    # if the random number is bigger than the threshold then work out an action from the network\n",
    "    if policy > 0.5:\n",
    "        # don't work out the gradient for this\n",
    "        with torch.no_grad():\n",
    "            # get the max column value, then gets its index and reshape into the expected format\n",
    "            return Q_net(state).max(1)[1].view(1, 1)\n",
    "    # as there is a return in the if statement we don't need an else. return a random number between 0-4 and put it in the right shaped tensor\n",
    "    return torch.tensor([[random.randrange(5)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 98359,
     "status": "ok",
     "timestamp": 1650813299951,
     "user": {
      "displayName": "Thomas Fishwick",
      "userId": "17375840183006086804"
     },
     "user_tz": -60
    },
    "id": "WvpHjhk9M4If",
    "outputId": "94c617c9-4fa7-43f1-db0a-beb53ad80067"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:139: UserWarning: Using a target size (torch.Size([256])) that is different to the input size (torch.Size([256, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, mean reward: -70.57281553398059\n",
      "Episode 2, mean reward: -100.50980392156863\n",
      "Episode 3, mean reward: -94.13725490196079\n",
      "Episode 4, mean reward: -100.0\n",
      "Episode 5, mean reward: -100.36633663366337\n",
      "Episode 6, mean reward: -101.58035714285714\n",
      "Episode 7, mean reward: -100.31683168316832\n",
      "Episode 8, mean reward: -83.12621359223301\n",
      "Episode 9, mean reward: -100.59803921568627\n",
      "Episode 10, mean reward: -100.0\n"
     ]
    }
   ],
   "source": [
    "# using https://spinningup.openai.com/en/latest/algorithms/sac.html\n",
    "\n",
    "# setup the networks and send them to the device\n",
    "Q_net = DQN().to(device)\n",
    "Q_target = DQN().to(device)\n",
    "# load the state dictionary of weights from the q network to the target\n",
    "Q_target.load_state_dict(Q_net.state_dict())\n",
    "# set the target to evaluation mode\n",
    "Q_target.eval()\n",
    "\n",
    "# setup the optimiser\n",
    "Q_optim = torch.optim.SGD(Q_net.parameters(), lr=0.01)\n",
    "\n",
    "Policy_net = Pi().to(device) # theta\n",
    "#Policy_target = Pi().to(device)\n",
    "#Policy_target.load_state_dict(Policy_net.state_dict())\n",
    "\n",
    "P_optim = torch.optim.SGD(Policy_net.parameters(), lr=0.01)\n",
    "\n",
    "# from table 1\n",
    "capacity = 10000\n",
    "discount = 0.99 # γ\n",
    "\n",
    "D = ReplayMemory(capacity)\n",
    "num_updates = 20\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "ALPHA = 1 # TODO find a less random number\n",
    "GAMMA = 0.9\n",
    "\n",
    "mean_rewards = []\n",
    "\n",
    "TARGET_UPDATE = 1\n",
    "\n",
    "for episode in range(1, 11):\n",
    "    # fill up buffer with random actions\n",
    "    D.reset()\n",
    "    Q_net.eval()\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    while len(D) < capacity:\n",
    "        # fill it up\n",
    "        # Get an instance of the map\n",
    "        amap = AdvancedMap()\n",
    "        # Get the normalised observations\n",
    "        obs, reward, done = amap.convert_observations(amap.reset())\n",
    "\n",
    "        # setup the total reward\n",
    "        total_reward = 0\n",
    "\n",
    "        steps_done = 0\n",
    "        Policy_net.eval()\n",
    "\n",
    "        # run until it is done\n",
    "        while not done:\n",
    "            # put the state into a tensor\n",
    "            state = torch.from_numpy(obs).unsqueeze(0)\n",
    "            \n",
    "            # pick an action and increment the number of steps\n",
    "            pol = Policy_net(state)\n",
    "            pol = pol.item()\n",
    "            action, steps_done = select_action(state, steps_done, pol), steps_done + 1\n",
    "            \n",
    "            # get the action\n",
    "            action = adv_action_from_index(action.item())\n",
    "\n",
    "            # take the action\n",
    "            next_state, reward, done = amap.convert_observations(amap.step(action))\n",
    "\n",
    "            # add to the reward\n",
    "            total_reward += reward\n",
    "\n",
    "            # set next state to nothing if we're done\n",
    "            if done:\n",
    "                next_state = None\n",
    "            \n",
    "            # add to the memories\n",
    "            # namedtuple('MDP_tuple', ['state', 'action', 'next_state', 'reward'])\n",
    "            D.push(obs, torch.tensor([action.index], device=device), next_state, torch.tensor([float(reward)], device=device), done)\n",
    "\n",
    "            # pass the next_state through to the next round\n",
    "            obs = next_state\n",
    "        rewards.append(total_reward)\n",
    "    # start updating\n",
    "    for j in range(num_updates):\n",
    "        Policy_net.train()\n",
    "        # get a batch of transitions from D\n",
    "        transitions = D.sample(BATCH_SIZE)\n",
    "        # convert the batch of transitions to a transition of batches\n",
    "        batch = MDP_tuple(*zip(*transitions))\n",
    "\n",
    "\n",
    "        # compute a mask of non-final states\n",
    "        #non_final_mask = torch.tensor(tuple(map(lambda state: state is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "\n",
    "        # get the next states\n",
    "        non_final_next_states = torch.cat([torch.tensor([s], device=device) for s in batch.next_state if s is not None])\n",
    "\n",
    "        state_batch = torch.cat([torch.tensor([s], device=device) for s in batch.state])\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        action_batch = action_batch.reshape((BATCH_SIZE, 1))\n",
    "\n",
    "        # compute target for the Q functions\n",
    "        y = torch.zeros(BATCH_SIZE, device=device)\n",
    "\n",
    "        targ = Q_target(non_final_next_states).min(1)[0].detach()\n",
    "        targ = torch.reshape(targ, (len(non_final_next_states), 1))\n",
    "        y = torch.reshape(targ - ALPHA * torch.log(Policy_net(non_final_next_states)), (len(non_final_next_states),))\n",
    "\n",
    "        # shape y to fill the gaps\n",
    "        ty = y.tolist()\n",
    "        lst = 0\n",
    "        y = []\n",
    "        for ns in batch.next_state:\n",
    "            if ns is None:\n",
    "                y.append(0)\n",
    "            else:\n",
    "                y.append(ty[lst])\n",
    "                lst = lst + 1\n",
    "        y = torch.tensor(y, device=device)\n",
    "        y = reward_batch + (GAMMA * y)\n",
    "\n",
    "        # Update Q-functions by one step of gradient descent\n",
    "        # work out the Q values\n",
    "        q_vals = Q_net(state_batch).gather(1, action_batch)\n",
    "        q_loss = torch.functional.F.mse_loss(q_vals, y)\n",
    "\n",
    "        # optimise the Q_model\n",
    "        Q_optim.zero_grad()\n",
    "        q_loss.backward()\n",
    "\n",
    "        # clip the gradients\n",
    "        for param in Q_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        Q_optim.step()\n",
    "\n",
    "        # Policy's turn (line 14 of https://spinningup.openai.com/en/latest/algorithms/sac.html#pseudocode)\n",
    "        # update policy by 1 step of gradient ascent\n",
    "        p_loss = torch.mean(Q_net(state_batch) - ALPHA * torch.log(Policy_net(state_batch)))\n",
    "\n",
    "        # optimise the policy model\n",
    "        P_optim.zero_grad()\n",
    "        p_loss.backward()\n",
    "        P_optim.step()\n",
    "    # append mean of rewards to mean_rewards\n",
    "    m_reward = np.mean(rewards)\n",
    "    mean_rewards.append(m_reward)\n",
    "    # update the target\n",
    "    if episode % TARGET_UPDATE == 0:\n",
    "        Q_target.load_state_dict(Q_net.state_dict())\n",
    "    print(f\"Episode {episode}, mean reward: {m_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 420,
     "status": "ok",
     "timestamp": 1650813342837,
     "user": {
      "displayName": "Thomas Fishwick",
      "userId": "17375840183006086804"
     },
     "user_tz": -60
    },
    "id": "A4YYUKNxI_2e",
    "outputId": "f489a2dd-b341-47a5-911a-9539408b3ad7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the rewards per episode\n",
    "plt.scatter(list(range(len(mean_rewards))), mean_rewards)\n",
    "plt.title(\"Mean rewards over time, Soft Actor Critic\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Mean rewards\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1650813459247,
     "user": {
      "displayName": "Thomas Fishwick",
      "userId": "17375840183006086804"
     },
     "user_tz": -60
    },
    "id": "gOswNjGYJNQb"
   },
   "outputs": [],
   "source": [
    "# Save the data\n",
    "with open(SAVE_PATH + 'data/SAC/rewards.pkl', 'wb') as f:\n",
    "    # serialise the rewards array\n",
    "    pickle.dump(mean_rewards, f)\n",
    "# save the state dict (from https://pytorch.org/tutorials/beginner/saving_loading_models.html)\n",
    "torch.save(Q_net.state_dict(), SAVE_PATH + 'data/SAC/qnet.pth')\n",
    "torch.save(Policy_net.state_dict(), SAVE_PATH + 'data/SAC/policynet.pth')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "SAC.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
