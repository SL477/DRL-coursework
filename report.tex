\documentclass[a4pape, 11pt, english]{article}
\usepackage[margin=1in,footskip=0.25in]{geometry}
\usepackage{natbib}

%\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\usepackage[hyphens]{url} % To wrap urls on multiple lines: https://tex.stackexchange.com/questions/115690/urls-in-bibliography-latex-not-breaking-line-as-expected

\begin{document}
\title{Deep Reinforcement Learning Project}
\author{Thomas Fishwick}

\maketitle

\begin{abstract}
TODO: Abstract
\end{abstract}

\section{Main problem: Starship Saboteur}
\subsection{Define an environment and the problem to be solved}
Here our agent will be teleported into a random free location on a spaceship (i.e. not blocked by any obstacles). Our agent will need to disable the spaceship's reactor by standing upon a certain location. Avoid the traps (which deal damage when stepped on) and navigate through the ship. Then go to the random beam-out location (which will also be an empty square) and escape before the max allowed turns, after that the ship will open the airlocks to immediately kill the agent.

The agent will be able to 'see' the area around it not blocked by walls/other obstacles. Doors will count as obstacles to vision, but not to the agent. Its next objective's direction will be fed to it, along with its current health.

\textbf{The story version:}
Fired from a Solar Federation Space Force Infiltrator, our intrepid Space Marine Commando robot exits its boarding pod in a random place in the Custodian warship. Its mission is to cripple the ship's reactor before its crew are revived from stasis. Its survival is optional, after it has completed its mission the robot can escape back to where a small ship has cut a hole into the ship to retrieve it.
% harpoon line has been fired into the ship to retrieve it.

\subsection{Define a state transition function and the reward function}

\subsection{Set up the Q-learning parameters (gamma, alpha) and policy}

\subsection{Run the Q-learning algorithm and represent its performance}

\subsection{Repeat the experiment with different parameter values, and policies}

\subsection{Analyse the results quantitatively and qualitatively}

\section{Implement DQN with two improvements}

\subsection{Analyse the results quantitatively and qualitatively}

\section{Apply the RL algorithm of your choice (from rllib) to one of the Atari Learning Environment. Briefly present the algorithm and justify your choice}

\subsection{Analyse the results quantitatively and qualitatively}

\section{Implementation of PPO or SAC}

\section{Summary of contribution}
100\% me

%\bibliographystyle{agsm} % using https://www.imperial.ac.uk/media/imperial-college/administration-and-support-services/library/public/LaTeX-example-Harvard-apr-2019.pdf to get Harvard style references
\bibliography{MyLibrary}
\end{document}