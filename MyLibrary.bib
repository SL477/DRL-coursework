
@book{wiering_reinforcement_2012,
	title = {Reinforcement {Learning}: {State}-of-the-{Art}},
	volume = {12},
	isbn = {978-3-642-27644-6},
	url = {https://r4.vlereader.com/Reader?ean=9783642276453},
	abstract = {Reinforcement learning encompasses both a science of adaptive behavior of rational beings in uncertain environments and a computational methodology for finding optimal behaviors for challenging problems in control, optimization and adaptive behavior of intelligent agents. As a field, reinforcement learning has progressed tremendously in the past decade.

The main goal of this book is to present an up-to-date series of survey articles on the main contemporary sub-fields of reinforcement learning. This includes surveys on partially observable environments, hierarchical task decompositions, relational knowledge representation and predictive state representations. Furthermore, topics such as transfer, evolutionary methods and continuous spaces in reinforcement learning are surveyed. In addition, several chapters review reinforcement learning methods in robotics, in games, and in computational neuroscience. In total seventeen different subfields are presented by mostly young experts in those areas, and together they truly represent a state-of-the-art of current reinforcement learning research.

Marco Wiering works at the artificial intelligence department of the University of Groningen in the Netherlands. He has published extensively on various reinforcement learning topics. Martijn van Otterlo works in the cognitive artificial intelligence group at the Radboud University Nijmegen in The Netherlands. He has mainly focused on expressive knowledge
representation in reinforcement learning settings.},
	language = {English},
	urldate = {2022-04-15},
	publisher = {Springer},
	author = {Wiering, Marco and van Otterlo, Martijn},
	year = {2012},
	file = {- VleReader:C\:\\Users\\link4\\Zotero\\storage\\EGF5DK2Q\\Reader.html:text/html},
}

@book{sutton_reinforcement_1998,
	address = {Cambridge},
	title = {Reinforcement {Learning}: {An} {Introduction}},
	isbn = {978-0-262-25705-3},
	shorttitle = {Reinforcement {Learning}},
	url = {https://muse.jhu.edu/book/60836},
	abstract = {Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications.Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
	urldate = {2022-04-15},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {1998},
}

@article{hessel_rainbow_2017,
	title = {Rainbow: {Combining} {Improvements} in {Deep} {Reinforcement} {Learning}},
	shorttitle = {Rainbow},
	url = {http://arxiv.org/abs/1710.02298},
	abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efﬁciency and ﬁnal performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
	language = {en},
	urldate = {2022-04-15},
	journal = {arXiv:1710.02298 [cs]},
	author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.02298},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Hessel et al. - 2017 - Rainbow Combining Improvements in Deep Reinforcem.pdf:C\:\\Users\\link4\\Zotero\\storage\\WKVF4W56\\Hessel et al. - 2017 - Rainbow Combining Improvements in Deep Reinforcem.pdf:application/pdf},
}

@article{van_hasselt_deep_2016,
	title = {Deep {Reinforcement} {Learning} with {Double} {Q}-{Learning}},
	volume = {30},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/10295},
	number = {1},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
	month = mar,
	year = {2016},
	file = {van Hasselt et al. - Deep Reinforcement Learning with Double Q-Learning.pdf:C\:\\Users\\link4\\Zotero\\storage\\F6ESWQ2H\\van Hasselt et al. - Deep Reinforcement Learning with Double Q-Learning.pdf:application/pdf},
}

@misc{schaul_prioritized_2015,
	title = {Prioritized {Experience} {Replay}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1511.05952},
	publisher = {arXiv},
	author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
	year = {2015},
	doi = {10.48550/ARXIV.1511.05952},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG)},
}
