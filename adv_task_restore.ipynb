{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore agent from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -U \"ray[rllib]\" torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import gym, ray\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "import ray.rllib.agents.dqn as dqn\n",
    "from helpers.advanced_map import AdvancedMap\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from helpers.r_matrix import r_matrix\n",
    "from helpers.get_available_actions import get_available_actions, adv_actions, adv_action_from_index\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.rllib.models import ModelCatalog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedMapEnv(gym.Env):\n",
    "  \"\"\"Class that wraps the advanced map to make it compatible with RLLib\n",
    "  \n",
    "  Functions:\n",
    "  __init__: takes in a config and creates the environment\n",
    "  \n",
    "  convert_observations: takes in a dictionary and returns a numpy array\n",
    "  \n",
    "  seed: set the random seed, with an optional integer\n",
    "  \n",
    "  step: take an action and see what happens\n",
    "  \n",
    "  reset: reset the environment back to the beginning\"\"\"\n",
    "  \n",
    "  def __init__(self, config: EnvContext):\n",
    "    \"\"\"Starts up the environment\n",
    "    \n",
    "    Inputs:\n",
    "    config: an EnvContext\"\"\"\n",
    "    # super(AdvancedMapEnv, self).__init__()\n",
    "    # create an advanced map object\n",
    "    self.advanced_map = AdvancedMap()\n",
    "\n",
    "    # Define the action space\n",
    "    # 5 actions: up, down, left, right, shoot\n",
    "    self.action_space = gym.spaces.Discrete(5)\n",
    "\n",
    "    # define the observation space, a 53 column box of floats between -1 and 1\n",
    "    self.observation_space = gym.spaces.Box(low=-1., high=1., shape=[53,], dtype=np.float32)\n",
    "  \n",
    "  def reset(self) -> np.array:\n",
    "      \"\"\"Reset the environment and send back the observations\"\"\"\n",
    "      # send through the reset command\n",
    "      obs = self.advanced_map.reset(pos=None)\n",
    "      # convert the observations\n",
    "      return self.convert_observations(obs)\n",
    "  \n",
    "  def step(self, action: int):\n",
    "    \"\"\"take the step, assuming that the action is valid\"\"\"\n",
    "    assert action in [0, 1, 2, 3, 4]\n",
    "\n",
    "    # get the action\n",
    "    actionstr = adv_action_from_index(action)\n",
    "\n",
    "    # pass in the step\n",
    "    obs = self.advanced_map.step(actionstr)\n",
    "\n",
    "    rew = obs['immediate_reward']\n",
    "\n",
    "    # return obs, reward and done\n",
    "    return self.convert_observations(obs), rew, obs['is_stop'], {}\n",
    "\n",
    "  def convert_observations(self, obs: dict) -> np.array:\n",
    "    \"\"\"Here we take in the dictionary of observations from the environment and returns a normalised numpy array\n",
    "    \n",
    "    The dictionary contains:\n",
    "    is_stop: a boolean, used internally to determine whether to stop the episode\n",
    "    immediate_reward: the points we got in the last round\n",
    "    enemy_count: how many active enemies there are\n",
    "    agent_view: the 7x7 view of the surroundings (values between 0-8)\n",
    "    obj_direction: the relative direction to the objective (0-max width, 0-max height)\n",
    "    agent_health: the health of the agent, between 0-100\"\"\"\n",
    "    # sort out the relative coordinates, so that the directions are divided by the size of the environment\n",
    "    obj_direction = np.divide(np.array(list(obs['obj_direction'])), np.array([21., 29.])) # TODO dynamically get the size\n",
    "\n",
    "    # normalise the view of the surroundings\n",
    "    agent_view = obs['agent_view'].ravel() / 8. # use ravel to reshape into a 1 row list and normalise it\n",
    "    \n",
    "    # enemy_count\n",
    "    enemy_count = np.array(obs['enemy_count'] / 3.) # TODO dynamically get the max number of enemies\n",
    "    \n",
    "    # agent_health\n",
    "    agent_health = np.array(obs['agent_health'] / 100.)\n",
    "    \n",
    "    # concatenate into a single numpy array, 1 row, 2 + 49 + 1 + 1 = 53 columns between -1 & 1\n",
    "    return np.concatenate([obj_direction, agent_view, [enemy_count], [agent_health]])\n",
    "\n",
    "    def seed(self, seed=None) -> None:\n",
    "      \"\"\"Set the random seed\"\"\"\n",
    "      random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-15 19:33:20,161\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "2022-04-15 19:33:20,162\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "Install gputil for GPU system monitoring.\n",
      "2022-04-15 19:33:23,668\tWARNING services.py:1983 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67035136 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=1.54gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2022-04-15 19:33:30,019\tINFO trainable.py:534 -- Restored on 172.17.0.2 from checkpoint: /root/ray_results/DQNTrainer_AdvancedMapEnv_2022-04-15_19-32-3355r71h0w/checkpoint_000001/checkpoint-1\n",
      "2022-04-15 19:33:30,021\tINFO trainable.py:543 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': 32, '_time_total': 2.429262399673462, '_episodes_total': 10}\n"
     ]
    }
   ],
   "source": [
    "# get the default config to use\n",
    "config = dqn.DEFAULT_CONFIG.copy()\n",
    "# use torch\n",
    "config['framework'] = 'torch'\n",
    "# set the environment\n",
    "config['env'] = AdvancedMapEnv\n",
    "# disable duelling\n",
    "config['dueling'] = False\n",
    "# disable double Q\n",
    "config['double_q'] = False\n",
    "# set the hidden units to 64 then 64\n",
    "#config['hiddens'] = [64, 64]\n",
    "# use relu (REctified Linear Units) activation\n",
    "config['model']['post_fcnet_activation'] = 'relu'\n",
    "config['model']['fcnet_hiddens'] = [256, 256]\n",
    "# set the gamma to 0.9, best parameter from the grid search\n",
    "config['gamma'] = 0.9\n",
    "\n",
    "# visualise what the agent is doing\n",
    "# using code from https://docs.ray.io/en/latest/rllib/rllib-training.html (computing actions)\n",
    "# setup environment\n",
    "env = AdvancedMapEnv(config)\n",
    "# setup the agent and restore from the checkpoint\n",
    "agent = dqn.DQNTrainer(config=config)\n",
    "# agent.load_checkpoint(\"data/test1/checkpoint-1\")\n",
    "agent.restore('/root/ray_results/DQNTrainer_AdvancedMapEnv_2022-04-15_19-32-3355r71h0w/checkpoint_000001/checkpoint-1')\n",
    "#agent.restore(\"/content/gdrive/My Drive/Colab Notebooks/DRL/DRL Coursework/data/test1/checkpoint-1\")\n",
    "#agent.restore(\"/root/ray_results/DQNTrainer_AdvancedMapEnv_2022-04-15_18-44-26rvaetiev/checkpoint_000001/checkpoint-1\")\n",
    "#agent.train()\n",
    "#agent.save()\n",
    "\n",
    "#training_loop(config, 'test1', 'test1', num_epochs=1, is_save=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
